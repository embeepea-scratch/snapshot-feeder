#! /usr/bin/python

# This script creates feed files for the images in the directory
# /var/www/Images.
# 
# Here's what it does, at the moment:
# 
# For each dataset_id in DATA_SOURCE_IDS:
#     Make a list of all the image (png or zip) files in all the RESDIRS under /var/www/Images/dataset_id
#     Eliminate any images that do not have a version present in all RESDIRS for this dataset_id
#     Write all the remaining images to a file named /var/www/Images/dataset_id/dataset_id--YYYY-MM-DD.csv, formatted as URLs
#     Write the file /var/www/Images/dataset_id/feed.csv containing the URL of the file (/var/www/Images/dataset_id/dataset_id--YYYY-MM-DD.csv)
#       written in the previous step
# 
# Currently, this results in all images for each dataset_id being
# listed in a single csv file, and that single csv file being listed
# in the dataset_id's feed.csv file.
# 
# Ultimately, the intention is to modify this script so that it
# creates a new csv file for each dataset_id each time it is run,
# containing only the images from that dataset_id that were not
# present during the previous run; and it appends the name of the new
# csv file to the dataset_id's feed.csv file.  That hasn't been
# implemneted yet, though.


import os, re, datetime, sys, csv, optparse

try:
    import Config
except:
    print """
Error: Config.py not found

  Feeder must be run from a directory containing a file named 'Config.py' which
  contains local configuration settings.  The file 'Config.tpl.py' which comes
  with feeder can be used as a template for creating a 'Config.py' file; see
  the instructions in 'Config.tpl.py' for details.

"""
    sys.exit(-1)

parser = optparse.OptionParser(usage='feeder [options]',
                               description='Creates and/or updates the master feed files and '
                               + 'snapshot feed files on a Data Snapshots image server.')
parser.add_option('--weblinks-only',
                  help="make sure that the `web` resolution directory links "
                  + "for all images on the server are present and correct, and "
                  + "remove any existing web links to non-existent images; does "
                  + "not create or update any feed files",
                  dest='weblinksOnly', default=False, action='store_true')
parser.add_option('--delete',
                  help="delete all feed files, including the master feed files; "
                  + "does not (re)generate any feed files -- only deletes existing ones.",
                  dest='delete', default=False, action='store_true')
parser.add_option('--limit-feed-lines',
                  help="put at most N lines into any one feed file; generates multiple feed "
                  + "files when images for more than N new dates are found for a data source.",
                  metavar='N', dest='feedLinesLimit', type='int', default=-1)
parser.add_option('--remaster',
                  help="regenerate all master feed files, ensuring that each data source's "
                  + "master feed file includes the URLs of all existing snapshot feed files "
                  + "for that data source.  Does not check for new images, or generate any "
                  + "new snapshot feed files, or modify any existing snapshot feed files in "
                  + "any way.",
                  dest='remaster', default=False, action='store_true')

(opts, args) = parser.parse_args()

def get_timestamp_string():
    """Return a string of the form YYYY-MM-DD-HH-mm-ss, representing the
    current date/time."""
    n = datetime.datetime.now()
    return "%04d-%02d-%02d--%02d-%02d-%02d" % (
        n.year,
        n.month+1,
        n.day,
        n.hour,
        n.minute,
        n.second)

date_generated = get_timestamp_string()

def snapshot_feed_header():
    """Return a string containing a comma-separated list of field names, which can be used as a header
    line in a CSV file; the string does NOT include a newline at the end."""
    return "guid,dsmn,ptk,stk,title,image_url,about,date_generated,download_title,download_url"

def snapshot_feed_line(dsmn, date, image_url, downloads):
    """Return a string containing a comma-separated list of field values for a single line in a CSV file,
    coresponding to a specific dsmn and date.  `dsmn` should be the id (machine name) of a data source,
    `date` should be a date in YYYY-MM-DD format, and `downloads` should be an array of all the download images
    for the given data source and date."""
    guid = dsmn + "-" + date
    ptk  = re.sub('-\d\d-\d\d$', '', date)
    stk  = re.sub('^\d\d\d\d-', '', date)
    title = dsmn + " for " + date
    about = ""
    download_title = ", ".join(Config.RESDIR_TITLES)
    download_url = ",".join([(Config.IMAGE_URL_PREFIX + file) for file in downloads])
    return ",".join( ('"' + f + '"') for f in [guid,
                                               dsmn,
                                               ptk,
                                               stk,
                                               title,
                                               image_url,
                                               about,
                                               date_generated,
                                               download_title,
                                               download_url]
                     )

def scan_res_images(dsmn,resdir):
    """Return a dict of all existing image files for a given data source
    and resolution.  dsmn is the machine name of the data source, and
    resdir is the resolution directory name.  Specifically, looks for
    files in IMAGE_ROOT/dsmn/resdir whose name matches the pattern
    *YYYY-MM-DD.SUFFIX, where SUFFIX is either 'png' or 'zip'.  The
    keys of the dict are strings of the form YYYY-MM-DD, and the value
    for each key is the corresponding filename.  Note that this does
    not allow for more than one file matching the pattern for a given
    YYYY-MM-DD value, but that's consistent with our conventions about
    how the images are stored."""
    imageDict = {}
    for file in os.listdir(os.path.join(Config.IMAGE_ROOT,dsmn,resdir)):
	m = re.search('(\d\d\d\d-\d\d-\d\d)\.((png)|(zip))$', file)
	if m:
            imageDict[m.group(1)] = file
    return imageDict

def scan_data_source_images(dsmn):
    """Scan all the resolution directories for a single data source, returning
    a dict containing information about all the images found therein.  The returned
    dict has the following structure:
      {
        "dsmn": "?????"                  # value of dsmn parameter passed in; a string
        "dates": [ "YYYY-MM-DD", ... ],  # sorted list of strings in the format YYYY-MM-DD, giving
                                         # the dates for which images are available for all resolutions
                                         # for this data source
        "imgs": {
          "1000" : {
              "YYYY-MM-DD" : "name-of-file-YYYY-MM-DD.png",
              "YYYY-MM-DD" : "name-of-file-YYYY-MM-DD.png",
              ...
          },
          "620" : {
              "YYYY-MM-DD" : "name-of-file-YYYY-MM-DD.png",
              "YYYY-MM-DD" : "name-of-file-YYYY-MM-DD.png",
              ...
          }
        }
      }
    Issues a warning if there are any dates for which images are present
    in some but not all of the resolution subdirs for the given data set.
    The returned "dates" array, as well as the "imgs" dicts containing
    the actual filenames, omits any dates not present in all resolutions."""
    imgs = {}
    datekeys = {}
    for resdir in Config.RESDIRS:
        imgs[resdir] = scan_res_images(dsmn,resdir)
        datekeys[resdir] = set( imgs[resdir].keys() )
    # set commonDates to the set of dates present in all img resdirs
    commonDates = datekeys[Config.RESDIRS[0]].intersection( *[datekeys[resdir] for resdir in Config.RESDIRS] )
    # check for images in each resdir whose dates are not in commonDates
    for resdir in Config.RESDIRS:
        extraImgs = datekeys[resdir].difference(commonDates)
        if len(extraImgs) > 0:
            print "Warning: %1d images found in %s/%s/%s for dates not present in all %s/%s/* resdirs" % (
                len(extraImgs), Config.IMAGE_ROOT, dsmn, resdir, Config.IMAGE_ROOT, dsmn)
            for img in extraImgs:
                print "    %s" % img
                del imgs[resdir][img]
    return {
        "dsmn"   : dsmn,
        "dates"  : sorted(commonDates),
        "imgs"   : imgs
    }


def scan_snapshot_feed_files(dsmn):
    """Return a set of strings giving the YYYY-MM-DD formatted time stamps
    of all images mentioned in all snapshot feed files present in the
    data source directory for a given data source.  dsmn is the machine
    name of the data source.  Scans the data source directory for all
    existing snapshot feed files and parses them all to construct the
    returned set."""
    dates = set()
    d = os.path.join(Config.FEED_ROOT,dsmn)
    if os.path.exists(d):
        for file in os.listdir(d):
            # only match csv files that contain YYYY-MM-DD date strings; this
            # ensures that we skip the master 'feed.csv' files:
            m = re.search('\d\d\d\d-\d\d-\d\d.*\.csv$', file)
            if m:
                with open(os.path.join(Config.FEED_ROOT,dsmn,file), 'r') as f:
                    csvreader = csv.reader(f)
                    csvreader.next() # skip header line
                    for row in csvreader:
                        date = "%s-%s" % (row[2], row[3])
                        dates.add(date)
    return dates

# guid,dsmn,ptk,stk,title,image_url,about,date_generated,download_title,download_url

def master_feed_file_path(dsmn):
    """Return the absolute path of the master feed file for a data source, as a string."""
    return os.path.join(Config.FEED_ROOT, dsmn, "feed.csv")

def append_to_master_feed_file(dsmn, line):
    """Append a single line to the master feed file for a data source; a newline
    will be written after the contents of the `line` argument.  The file is
    opened for appending, the line is written, and the file closed, before this
    function returns."""
    with open(master_feed_file_path(dsmn), "a") as f:
        f.write(line + "\n")
    print "Updated %s" % master_feed_file_path(dsmn)

def web_link_name(dsmn, date):
    """Construct the name of the web link for an image, based on data source
    machine name and date; returns a string."""
    return "%s--web--%s.png" % (dsmn, date)

def ensure_directory(dirname):
    """Create directory `dirname` if it does not already exist."""
    if not os.path.exists(dirname):
            os.mkdir(dirname)

def create_web_link(dsmn, date, imgs):
    """Create a symlink to the `620` resolution version of an image in the
    `web` resolution directory.  Creates the web resolution directory
    if it does not already exist.  Returns the URL of the created
    link.  `dsmn` is the machine name of a data source, `date` is a
    YYYY-MM-DD string, and `imgs` is a nested dictionary giving the
    names of the existing images for the data source, keyed by
    resolution first, then date (`imgs` is the value of the "imgs" key
    in the dictionary returned by scan_data_source_images().  Works
    non-destructively, meaning that if the link already exists, and
    points to the correct `620` resolution version of the image, does
    not delete or recreate the link, but still returns the URL of the
    link."""
    web_res_dir = os.path.join(Config.IMAGE_ROOT, dsmn, "web")
    ensure_directory(web_res_dir)
    web_png = web_link_name(dsmn, date)
    web_png_path = os.path.join(web_res_dir, web_png)
    linkpath = os.path.join("..", "620", imgs["620"][date])
    needlink = False
    if os.path.exists(web_png_path):
        oldpath = os.readlink(web_png_path)
        if oldpath != linkpath:
            os.remove(web_png_path)
            needlink = true
    if needlink:
        os.symlink(linkpath, web_png_path)
    return Config.IMAGE_URL_PREFIX + os.path.join(dsmn, 'web', web_png)

def n_chunk(lst,n):
    """Break a list `lst` into chunks of size at most `n`; returns a list of lists."""
    for i in xrange(0, len(lst), n):
        yield lst[i:i+n]

def write_snapshot_feed_file(dsmn, name_base, name_suffix, dates, imgs):
    snapshot_feed_file = os.path.join(dsmn, "%s%s.csv" % (name_base, name_suffix))
    nimages = 0
    with open(os.path.join(Config.FEED_ROOT, snapshot_feed_file), "w") as f:
        f.write(snapshot_feed_header() + "\n")
        for date in dates:
            web_image_url = create_web_link(dsmn, date, imgs)
            f.write(snapshot_feed_line(dsmn, date, web_image_url,
                                       [os.path.join(dsmn, res, imgs[res][date]) for res in Config.RESDIRS]
                                   ) + "\n")
            nimages += 1
    print "Wrote %1d images to %s" % (nimages, os.path.join(Config.FEED_ROOT, snapshot_feed_file))
    append_to_master_feed_file(dsmn, Config.FEED_URL_PREFIX + snapshot_feed_file)


def process_data_source(dsmn):
    images = scan_data_source_images(dsmn)
    imgs = images['imgs']
    dates_already_processed = scan_snapshot_feed_files(dsmn)
    new_dates = set(images["dates"])
    # treat the "0000-00-00" date specially
    if "0000-00-00" in new_dates:
        create_web_link(dsmn, "0000-00-00", imgs) # create the web link for it
        new_dates.remove("0000-00-00") # remove it from further processing
    # eliminate dates already processed:
    new_dates = sorted(new_dates.difference(dates_already_processed))
    # process new dates:
    if new_dates:
        ensure_directory(os.path.join(Config.FEED_ROOT, dsmn))
        if opts.feedLinesLimit > -1 and len(new_dates) > opts.feedLinesLimit:
            chunk_i = 0
            for chunk in n_chunk(new_dates, opts.feedLinesLimit):
                write_snapshot_feed_file(dsmn, dsmn + "--" + date_generated,
                                         ".%03d" % chunk_i, chunk, imgs)
                chunk_i += 1
        else:
            write_snapshot_feed_file(dsmn, dsmn + "--" + date_generated,
                                     "", new_dates, imgs)

def recreate_web_links(dsmn):
    """Update the web links for all images for a data source; make sure that links are correct
    for all images present, and remove any links that point to non-existent images."""
    images = scan_data_source_images(dsmn)
    imgs = images['imgs']
    # make sure that web links for all current images are present:
    for date in images['dates']:
        create_web_link(dsmn, date, imgs)
    # remove any web links that point to non-existent images
    for orphan_date in set(scan_res_images(dsmn,'web').keys()).difference(set(images['dates'])):
        os.remove(os.path.join(Config.IMAGE_ROOT, dsmn, 'web', web_link_name(dsmn,orphan_date)))
        print "removing orphan link: %s" % orphan_date

def delete_feed_files(dsmn):
    """Delete all the feed files for a data source."""
    d = os.path.join(Config.FEED_ROOT, dsmn)
    for f in os.listdir(d):
	m = re.search('\.csv$', f)
	if m:
            os.remove(os.path.join(d, f))

def remaster(dsmn):
    """Regenerate the master feed file for the given data source, making sure
    it contains the URLs for all current snapshot feed files for that data source."""
    d = os.path.join(Config.FEED_ROOT, dsmn)
    files = []
    for file in os.listdir(d):
	m = re.search('\.csv$', file)
	if m and file != "feed.csv":
            files.append(file)
    master_feed_file = os.path.join(Config.FEED_ROOT, dsmn, "feed.csv")
    with open(master_feed_file, "w") as f:
        for file in sorted(files):
            f.write(Config.FEED_URL_PREFIX + dsmn + "/" + file + "\n")
    print "Wrote %s" % master_feed_file

if opts.delete:
    for dsmn in Config.DATA_SOURCE_IDS:
        delete_feed_files(dsmn)
    sys.exit(0)

for dsmn in Config.DATA_SOURCE_IDS:
    if opts.weblinksOnly:
        recreate_web_links(dsmn)
    elif opts.remaster:
        remaster(dsmn)
    else:
        process_data_source(dsmn)
